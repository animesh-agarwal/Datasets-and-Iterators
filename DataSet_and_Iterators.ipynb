{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataSet and Iterators.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MZoPyp4kRbls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having efficient data pipelines is of paramount importance for any machine learning model. In this blog we will learn how to use TensorFlow's Dataset module tf.data to build efficient data pipelines."
      ]
    },
    {
      "metadata": {
        "id": "MUwBuGQRRdlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most of the introductory articles on TensorFlow would introduce you with the feed_dict method of feeding the data to the model. feed_dict processes the input data in a single thread and while the data is being loaded and processed on CPU, the GPU remains idle and when the GPU is training the first batch of data, CPU remains in idle state. The developers of TensorFlow have advised not to use this method during training or repeated validation of same datasets."
      ]
    },
    {
      "metadata": {
        "id": "jCkKNnDERggP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "tf_data improves the performance by prefetching the next batch of data asynchronously so that GPU need not wait for the data. You can also parallelize the process of preprocessing and loading the dataset."
      ]
    },
    {
      "metadata": {
        "id": "zG3wfcLS_xaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vcdIKXHimAFq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Creating Datasets"
      ]
    },
    {
      "metadata": {
        "id": "IBLL4pzuucQ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensorflow provides many methods to create datasets from numpy arrays, tensors, text files, csv files, etc. Let's have a look at all the methods. \n",
        "\n",
        "1. **from_tensor_slices** Accepts single or multiple numpy/tensor objects. This emits only one data at a time when iterator's get_next is called"
      ]
    },
    {
      "metadata": {
        "id": "bE8zTEXmA9sA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# source data - numpy array \n",
        "data = np.arange(0,10)\n",
        "\n",
        "# create a dataset from numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HlYewYDBrI8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The object **dataset** is a tensorflow Dataset object. We need to create an iterator that will extract data from this dataset."
      ]
    },
    {
      "metadata": {
        "id": "YA4mPecEBeFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwFW62UNCeFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The iterator is not aware of the number of elements in the dataset. The get_next function returns the next element in the dataset until it is exhausted. Once exhausted it will throw **tf.errors.OutOfRangeError** exception. The below code will print out integers from 0 to 9. If we try to run the **next_element** once the dataset is exhausted,  it will throw an OutOfRangeError. This is because the code extracted all the data slices from the dataset and it is now out of range or \"empty\"\n"
      ]
    },
    {
      "metadata": {
        "id": "fHjmyHGDCDBg",
        "colab_type": "code",
        "outputId": "fb48d186-55f3-44bc-943e-c049ab0692fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  for i in range(10):\n",
        "    val = sess.run(next_element)\n",
        "    print(val)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kOcYhpKfpFC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observe that get_next() returns only one element at a time. To fetch all the data from the dataset, we need to run the get_next function for times equal to the number of elements in the dataset. \n",
        "\n",
        "You can also pass multiple numpy arrays to the dataset. Example: In case of training a model, we would need the pair of features and labels"
      ]
    },
    {
      "metadata": {
        "id": "ATY4kp4cqoZq",
        "colab_type": "code",
        "outputId": "9c6ca63c-dd4f-4b6d-806f-b650fe21091f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "features, labels = (np.random.uniform((100,2)), np.random.uniform((100,1)))\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  val = sess.run(next_element)\n",
        "  print(val)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62.6666235097523, 48.174537150505884)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NkTfmpGWpqAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. **from_tensors** This method also accepts multipe numpy arrays and tensors, but emits all the data at once"
      ]
    },
    {
      "metadata": {
        "id": "uGfK7tNeppgB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = np.arange(10,15)\n",
        "dataset = tf.data.Dataset.from_tensors(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MZUbmVG_o6K_",
        "colab_type": "code",
        "outputId": "5cff85f4-824f-4260-910b-9190975a3454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  val = sess.run(next_element)\n",
        "  print(val)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10 11 12 13 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P050g2S8qXea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observe that all the values in the dataset are emitted at once"
      ]
    },
    {
      "metadata": {
        "id": "UiCYJblqsz5u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. **from_generator** This method takes a generator function as an input. This emits one data at a time"
      ]
    },
    {
      "metadata": {
        "id": "c2WLvbZHCRLW",
        "colab_type": "code",
        "outputId": "862b8e6c-80e1-4bea-8586-4c100b80170c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "cell_type": "code",
      "source": [
        "def generator():\n",
        "  for i in range(10):\n",
        "    yield 2*i\n",
        "    \n",
        "dataset = tf.data.Dataset.from_generator(generator, (tf.int32))\n",
        "\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  for i in range(10):\n",
        "    val = sess.run(next_element)\n",
        "    print(val)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cAlWSTeNuC9Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Transforming of Datasets"
      ]
    },
    {
      "metadata": {
        "id": "XCoND1qNpcUF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Batches** - Combines consecutive elements of the dataset into a single batch. Useful when you want to train smaller batches of data while training the model to avoid out of memory errors."
      ]
    },
    {
      "metadata": {
        "id": "L7VdyBQPpbMC",
        "colab_type": "code",
        "outputId": "a02787ec-34b6-469f-feb3-8761ae4b6655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "data = np.arange(10,40)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data).batch(10)\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_ele = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10 11 12 13 14 15 16 17 18 19]\n",
            "[20 21 22 23 24 25 26 27 28 29]\n",
            "[30 31 32 33 34 35 36 37 38 39]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I05_JsQHr2S1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Zip** - Creates a dataset by zipping together datasets. Useful in scenarios where you have features and labels and you need to provide the pair of feature and label for training the model."
      ]
    },
    {
      "metadata": {
        "id": "c4ba-_VAr7bD",
        "colab_type": "code",
        "outputId": "4aee8050-0453-42b3-d9f6-7417cf59b861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "datax = np.arange(10,20)\n",
        "datay = np.arange(11,21)\n",
        "\n",
        "datasetx = tf.data.Dataset.from_tensor_slices(datax)\n",
        "datasety = tf.data.Dataset.from_tensor_slices(datay)\n",
        "\n",
        "dcombined = tf.data.Dataset.zip((datasetx, datasety)).batch(2)\n",
        "iterator = dcombined.make_one_shot_iterator()\n",
        "next_ele = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([10, 11]), array([11, 12]))\n",
            "(array([12, 13]), array([13, 14]))\n",
            "(array([14, 15]), array([15, 16]))\n",
            "(array([16, 17]), array([17, 18]))\n",
            "(array([18, 19]), array([19, 20]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2oTKzqPeUEs_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Repeat**"
      ]
    },
    {
      "metadata": {
        "id": "WC6004VOUETP",
        "colab_type": "code",
        "outputId": "971282b1-bfed-4c21-82e2-e9ba5965a9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "dataset = dataset.repeat(2)\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_ele = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "csgUlpNku5vy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Map** - Used to transform the elements of the dataset. Useful in cases where you want to transform your raw data before feeding into the model."
      ]
    },
    {
      "metadata": {
        "id": "ALEcZMMUuOAz",
        "colab_type": "code",
        "outputId": "cf01ba8a-1a4a-49bc-df2d-3372d94c8b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "def map_fnc(x):\n",
        "  return x*2;\n",
        "\n",
        "data = np.arange(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "dataset = dataset.map(map_fnc)\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_ele = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "\n",
        "  \n",
        "\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZSaTzomyT4aP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Creating Iterators"
      ]
    },
    {
      "metadata": {
        "id": "2s0cPkdlT8bU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**One-shot iterator:** This is the most basic form of iterator. It requires no explicit initialization and iterates over the data only one time and once it gets exhausted, it cannot be re-initialized."
      ]
    },
    {
      "metadata": {
        "id": "XxZ95XaeT_SW",
        "colab_type": "code",
        "outputId": "fb8ac495-4f13-45ca-c23d-5ba0959e0290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = np.arange(10,15)\n",
        "#create the dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "#create the iterator\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  val = sess.run(next_element)\n",
        "  print(val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4lDSooJ_UDur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initializable iterator:** This iterator requires you to explicitly initialize the iterator by running iterator.initialize. You can define a tf.placeholder and pass data to it dynamically each time you call the initialize operation."
      ]
    },
    {
      "metadata": {
        "id": "q_J9dbfUUIXd",
        "colab_type": "code",
        "outputId": "85328591-7f66-463c-cfe6-7c389f294046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "# define two placeholders to accept min and max value\n",
        "min_val = tf.placeholder(tf.int32, shape=[])\n",
        "max_val = tf.placeholder(tf.int32, shape=[])\n",
        "data = tf.range(min_val, max_val)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "next_ele = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  print(\"------Dataset1-----\")\n",
        "  # initialize an iterator with range of values from 10 to 15\n",
        "  sess.run(iterator.initializer, feed_dict={min_val:10, max_val:15})\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "     \n",
        "  print(\"\\n\")  \n",
        "  print(\"------Dataset2-----\")\n",
        "  # initialize an iterator with range of values from 1 to 10\n",
        "  sess.run(iterator.initializer, feed_dict={min_val:1, max_val:10})\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "------Dataset1-----\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "\n",
            "\n",
            "------Dataset2-----\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E-C00XKSUNGG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reinitializable iterator:** This iterator can be initialized from different Dataset objects that have the same structure. Each dataset can pass through it's own transformation pipeline."
      ]
    },
    {
      "metadata": {
        "id": "gXdSlMr8UXMO",
        "colab_type": "code",
        "outputId": "2e78f0c9-6bba-4ec3-e775-b3c48e1c1f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "def map_fnc(ele):\n",
        "  return ele*2\n",
        "min_val = tf.placeholder(tf.int32, shape=[])\n",
        "max_val = tf.placeholder(tf.int32, shape=[])\n",
        "data = tf.range(min_val, max_val)\n",
        "#Define separate datasets for training and validation\n",
        "train_dataset =  tf.data.Dataset.from_tensor_slices(data)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(data).map(map_fnc)\n",
        "#create an iterator \n",
        "iterator=tf.data.Iterator.from_structure(train_dataset.output_types    ,train_dataset.output_shapes)\n",
        "train_initializer = iterator.make_initializer(train_dataset)\n",
        "val_initializer = iterator.make_initializer(val_dataset)\n",
        "next_ele = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  # initialize an iterator with range of values from 10 to 15\n",
        "  print(\"-----Training-----\")\n",
        "  sess.run(train_initializer, feed_dict={min_val:10, max_val:15})\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"-------Validation------\")\n",
        "  # initialize an iterator with range of values from 1 to 10\n",
        "  sess.run(val_initializer, feed_dict={min_val:1, max_val:10})\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele)\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----Training-----\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "\n",
            "\n",
            "-------Validation------\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cT9-28ynUYzR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Feedable iterator:** Can be used to switch between Iterators for different datasets. Useful when you have different datasets and you want to have more control over which iterator to use over the dataset."
      ]
    },
    {
      "metadata": {
        "id": "kKx-WfP1UcIs",
        "colab_type": "code",
        "outputId": "209de5c0-be1a-4f25-c30b-be61b20b993d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "cell_type": "code",
      "source": [
        "def map_fnc(ele):\n",
        "  return ele*2\n",
        "min_val = tf.placeholder(tf.int32, shape=[])\n",
        "max_val = tf.placeholder(tf.int32, shape=[])\n",
        "data = tf.range(min_val, max_val)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(data).map(map_fnc)\n",
        "train_val_iterator = tf.data.Iterator.from_structure(train_dataset.output_types , train_dataset.output_shapes)\n",
        "train_initializer = train_val_iterator.make_initializer(train_dataset)\n",
        "val_initializer = train_val_iterator.make_initializer(val_dataset)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(tf.range(10,15))\n",
        "test_iterator = test_dataset.make_one_shot_iterator()\n",
        "handle = tf.placeholder(tf.string, shape=[])\n",
        "iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n",
        "next_ele = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  train_val_handle = sess.run(train_val_iterator.string_handle())\n",
        "  test_handle = sess.run(test_iterator.string_handle())\n",
        "  \n",
        "  print(\"-----Training------\")\n",
        "  # training\n",
        "  sess.run(train_initializer, feed_dict={min_val:10, max_val:15})\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele, feed_dict={handle:train_val_handle})\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"------Validation-----\")\n",
        "  #validation\n",
        "  sess.run(val_initializer, feed_dict={min_val:1, max_val:10})\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele, feed_dict={handle:train_val_handle})\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"------Testing-------\")\n",
        "  #testing\n",
        "  try:\n",
        "    while True:\n",
        "      val = sess.run(next_ele, feed_dict={handle:test_handle})\n",
        "      print(val)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----Training------\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "\n",
            "\n",
            "------Validation-----\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n",
            "\n",
            "\n",
            "------Testing-------\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "emnUueksUikW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Building the LeNet5 Model"
      ]
    },
    {
      "metadata": {
        "id": "ZstwJdnCUmDD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's import the MNIST data from the tensorflow library. The MNIST database contains 60,000 training images and 10,000 testing images. "
      ]
    },
    {
      "metadata": {
        "id": "tgbF1bu9Uhjc",
        "colab_type": "code",
        "outputId": "e106c845-3e50-4840-e2be-158979861ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, one_hot = True)\n",
        "\n",
        "X_train, y_train = mnist.train.images, mnist.train.labels\n",
        "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test = mnist.test.images, mnist.test.labels\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-f1d0402b3ef1>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ICqRNaBqdDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = np.pad(X_train, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
        "X_val =   np.pad(X_val, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
        "X_test =  np.pad(X_test, ((0,0), (2,2), (2,2), (0,0)), 'constant')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7NSbP_eSq9Eg",
        "colab_type": "code",
        "outputId": "714e5fd9-8489-407e-cdf4-20225cab2222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55000, 32, 32, 1)\n",
            "(55000, 10)\n",
            "(5000, 32, 32, 1)\n",
            "(5000, 10)\n",
            "(10000, 32, 32, 1)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmAKPWs4tQkl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_pass(X):\n",
        "    W1 = tf.get_variable(\"W1\", [5,5,1,6], initializer = tf.contrib.layers.xavier_initializer(seed=0))\n",
        "    # for conv layer2\n",
        "    W2 = tf.get_variable(\"W2\", [5,5,6,16], initializer = tf.contrib.layers.xavier_initializer(seed=0))\n",
        "    Z1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding='VALID')\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    P1 = tf.nn.max_pool(A1, ksize = [1,2,2,1], strides = [1,2,2,1], padding='VALID')\n",
        "    Z2 = tf.nn.conv2d(P1, W2, strides = [1,1,1,1], padding='VALID')\n",
        "    A2= tf.nn.relu(Z2)\n",
        "    P2= tf.nn.max_pool(A2, ksize = [1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    P2 = tf.contrib.layers.flatten(P2)\n",
        "   \n",
        "    Z3 = tf.contrib.layers.fully_connected(P2, 120)\n",
        "    Z4 = tf.contrib.layers.fully_connected(Z3, 84)\n",
        "    Z5 = tf.contrib.layers.fully_connected(Z4,10, activation_fn= None)\n",
        "    return Z5\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB2_HfBEuZYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X,Y):\n",
        "    \n",
        "    logits = forward_pass(X)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0009)\n",
        "    learner = optimizer.minimize(cost)\n",
        "    correct_predictions = tf.equal(tf.argmax(logits,1), tf.argmax(Y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    \n",
        "    return (learner, accuracy)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Wr7QRBUU0XK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have now created the model. Before deciding on the Iterator to use for our model, let's see what are the typical requirements of a machine learning model.\n",
        "\n",
        "\n",
        "*   **Training the data over batches:** Dataset can be very huge. To prevent out of memory errors, we would need to train our dataset in small batches.\n",
        "*   **Train the model over n passes of the dataset:** Typically you want to run your training model over multiple passes of the dataset.\n",
        "*   **Validate the model at each epoch:** You would need to validate your model at each epoch to check your model's performance.\n",
        "*   **Finally test your model on unseen data:** After the model is trained, you would like to test your model on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "Let's see the pros and cons of each iterator."
      ]
    },
    {
      "metadata": {
        "id": "0fJmor9IKZSu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##One Shot Iterator"
      ]
    },
    {
      "metadata": {
        "id": "5R0lUWCFVUsb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Dataset can't be reinitialized once exhausted. To train for more epochs, you would need to repeat the Dataset before feeding to the iterator. This will require huge memory if the size of the data is large. It also doesn't provide any option to validate the model."
      ]
    },
    {
      "metadata": {
        "id": "xtkfKO0mKY6v",
        "colab_type": "code",
        "outputId": "04078652-b641-4d27-b0e9-20d5b17c20bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10 \n",
        "batch_size = 64 \n",
        "iterations = len(y_train) * epochs \n",
        "\n",
        "tf.reset_default_graph()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "# need to repeat the dataset for epoch number of times, as all the data needs\n",
        "# to be fed to the dataset at once\n",
        "dataset = dataset.repeat(epochs).batch(batch_size)\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "\n",
        "X_batch , Y_batch = iterator.get_next()\n",
        "\n",
        "\n",
        "(learner, accuracy) = model(X_batch, Y_batch)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  total_accuracy = 0\n",
        "  try:\n",
        "    while True:\n",
        "      temp_accuracy, _ = sess.run([accuracy, learner])\n",
        "      total_accuracy += temp_accuracy\n",
        "      \n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "  \n",
        "  \n",
        "print('Avg training accuracy is {}'.format((total_accuracy * batch_size) / iterations ))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "Avg training accuracy is 0.9822563636363636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2TRPHOTBTdFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Initializable iterator"
      ]
    },
    {
      "metadata": {
        "id": "ytejPDd4VajN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can dynamically change the Dataset between training and validation Datasets at the time of initializing the iterator. However both the Dataset needs to go through the same transformation pipeline."
      ]
    },
    {
      "metadata": {
        "id": "3PrRgZqyNol1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "029b4ded-6ce2-4e5c-c495-864ef5b62539"
      },
      "cell_type": "code",
      "source": [
        "epochs = 10 \n",
        "batch_size = 64 \n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X_data = tf.placeholder(tf.float32, [None, 32,32,1])\n",
        "Y_data = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data))\n",
        "dataset = dataset.batch(batch_size)\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "\n",
        "X_batch , Y_batch = iterator.get_next()\n",
        "\n",
        "\n",
        "(learner, accuracy) = model(X_batch, Y_batch)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # train the model\n",
        "    sess.run(iterator.initializer, feed_dict={X_data:X_train, Y_data:y_train})\n",
        "    total_train_accuracy = 0\n",
        "    no_train_examples = len(y_train)\n",
        "    try:\n",
        "      while True:\n",
        "        temp_train_accuracy, _ = sess.run([accuracy, learner])\n",
        "        total_train_accuracy += temp_train_accuracy*batch_size\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    # validate the model\n",
        "    sess.run(iterator.initializer, feed_dict={X_data:X_val, Y_data:y_val})\n",
        "    total_val_accuracy = 0\n",
        "    no_val_examples = len(y_val)\n",
        "    try:\n",
        "      while True:\n",
        "        temp_val_accuracy = sess.run(accuracy)\n",
        "        total_val_accuracy += temp_val_accuracy*batch_size\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    print('Epoch {}'.format(str(epoch+1)))\n",
        "    print(\"---------------------------\")\n",
        "    print('Training accuracy is {}'.format(total_train_accuracy/no_train_examples))\n",
        "    print('Validation accuracy is {}'.format(total_val_accuracy/no_val_examples))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Training accuracy is 0.9227272727272727\n",
            "Validation accuracy is 0.9748\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Training accuracy is 0.9742181818181819\n",
            "Validation accuracy is 0.9914\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Training accuracy is 0.9823454545454545\n",
            "Validation accuracy is 0.9954\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Training accuracy is 0.9876\n",
            "Validation accuracy is 0.997\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Training accuracy is 0.9903454545454545\n",
            "Validation accuracy is 0.999\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Training accuracy is 0.9924727272727273\n",
            "Validation accuracy is 0.998\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Training accuracy is 0.9940727272727272\n",
            "Validation accuracy is 0.999\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Training accuracy is 0.9940909090909091\n",
            "Validation accuracy is 0.997\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Training accuracy is 0.9954\n",
            "Validation accuracy is 0.9988\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Training accuracy is 0.9958\n",
            "Validation accuracy is 0.9962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YulQh4t2jDnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Reinitializable Iterartor"
      ]
    },
    {
      "metadata": {
        "id": "BUvvTUl6VfWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This iterator overcomes the problem of initializable iterator by using two separate Datasets. Each dataset can go through it's own preprocessing pipleine."
      ]
    },
    {
      "metadata": {
        "id": "DOmnGlmDjH8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "dd5f976f-c3ae-4f1f-f97c-bd5276559ddc"
      },
      "cell_type": "code",
      "source": [
        "epochs = 10 \n",
        "batch_size = 64 \n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X_data = tf.placeholder(tf.float32, [None, 32,32,1])\n",
        "Y_data = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data)).batch(batch_size)\n",
        "val_dataset =  tf.data.Dataset.from_tensor_slices((X_data, Y_data)).batch(batch_size)\n",
        "\n",
        "\n",
        "iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
        "X_batch , Y_batch = iterator.get_next()\n",
        "(learner, accuracy) = model(X_batch, Y_batch)\n",
        "\n",
        "train_initializer = iterator.make_initializer(train_dataset)\n",
        "val_initializer =  iterator.make_initializer(val_dataset)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # train the model\n",
        "    sess.run(train_initializer, feed_dict={X_data:X_train, Y_data:y_train})\n",
        "    total_train_accuracy = 0\n",
        "    no_train_examples = len(y_train)\n",
        "    try:\n",
        "      while True:\n",
        "        temp_train_accuracy, _ = sess.run([accuracy, learner])\n",
        "        total_train_accuracy += temp_train_accuracy*batch_size\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    # validate the model\n",
        "    sess.run(val_initializer, feed_dict={X_data:X_val, Y_data:y_val})\n",
        "    total_val_accuracy = 0\n",
        "    no_val_examples = len(y_val)\n",
        "    try:\n",
        "      while True:\n",
        "        temp_val_accuracy = sess.run(accuracy)\n",
        "        total_val_accuracy += temp_val_accuracy*batch_size\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    print('Epoch {}'.format(str(epoch+1)))\n",
        "    print(\"---------------------------\")\n",
        "    print('Training accuracy is {}'.format(total_train_accuracy/no_train_examples))\n",
        "    print('Validation accuracy is {}'.format(total_val_accuracy/no_val_examples))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Training accuracy is 0.9220909090909091\n",
            "Validation accuracy is 0.9682\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Training accuracy is 0.9752545454545455\n",
            "Validation accuracy is 0.9916\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Training accuracy is 0.9830181818181818\n",
            "Validation accuracy is 0.9928\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Training accuracy is 0.9871818181818182\n",
            "Validation accuracy is 0.9964\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Training accuracy is 0.9907454545454546\n",
            "Validation accuracy is 0.9968\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Training accuracy is 0.9925818181818182\n",
            "Validation accuracy is 0.9972\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Training accuracy is 0.9937272727272727\n",
            "Validation accuracy is 0.9966\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Training accuracy is 0.9942909090909091\n",
            "Validation accuracy is 0.9958\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Training accuracy is 0.9953636363636363\n",
            "Validation accuracy is 0.9988\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Training accuracy is 0.9960363636363636\n",
            "Validation accuracy is 0.996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6yL-2kqnihA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Feedable Iterator"
      ]
    },
    {
      "metadata": {
        "id": "Oui31HXNVkDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This iterator provides the option of switching between the iterators. In my opinion this is the best. You can create a reinitializable iterator for training and validation purposes. For inference/testing where you require one pass of the dataset, you can use the one shot iterator."
      ]
    },
    {
      "metadata": {
        "id": "HU5ElY0wnptC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "a38365db-71cd-4c33-a371-9d8ce5b87a12"
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 64 \n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X_data = tf.placeholder(tf.float32, [None, 32,32,1])\n",
        "Y_data = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data)).batch(batch_size)\n",
        "val_dataset =  tf.data.Dataset.from_tensor_slices((X_data, Y_data)).batch(batch_size)\n",
        "\n",
        "test_dataset =  tf.data.Dataset.from_tensor_slices((X_test, y_test.astype(np.float32))).batch(batch_size)\n",
        "\n",
        "\n",
        "handle = tf.placeholder(tf.string, shape=[])\n",
        "iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n",
        "X_batch , Y_batch = iterator.get_next()\n",
        "(learner, accuracy) = model(X_batch, Y_batch)\n",
        "\n",
        "train_val_iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
        "train_iterator = train_val_iterator.make_initializer(train_dataset)\n",
        "val_iterator = train_val_iterator.make_initializer(val_dataset)\n",
        "\n",
        "test_iterator = test_dataset.make_one_shot_iterator()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  train_val_string_handle = sess.run(train_val_iterator.string_handle())\n",
        "  test_string_handle = sess.run(test_iterator.string_handle())\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # train the model\n",
        "    sess.run(train_iterator, feed_dict={X_data:X_train, Y_data:y_train})\n",
        "    total_train_accuracy = 0\n",
        "    no_train_examples = len(y_train)\n",
        "    try:\n",
        "      while True:\n",
        "        temp_train_accuracy, _ = sess.run([accuracy, learner], feed_dict={handle:train_val_string_handle})\n",
        "        total_train_accuracy += temp_train_accuracy*batch_size\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    # validate the model\n",
        "    sess.run(val_iterator, feed_dict={X_data:X_val, Y_data:y_val})\n",
        "    total_val_accuracy = 0\n",
        "    no_val_examples = len(y_val)\n",
        "    try:\n",
        "      while True:\n",
        "        temp_val_accuracy = sess.run(accuracy, feed_dict={handle:train_val_string_handle})\n",
        "        total_val_accuracy += temp_val_accuracy*batch_size\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    \n",
        "    print('Epoch {}'.format(str(epoch+1)))\n",
        "    print(\"---------------------------\")\n",
        "    print('Training accuracy is {}'.format(total_train_accuracy/no_train_examples))\n",
        "    print('Validation accuracy is {}'.format(total_val_accuracy/no_val_examples))\n",
        "  \n",
        "  \n",
        "  print(\"Testing the model --------\")\n",
        " \n",
        "  total_test_accuracy = 0\n",
        "  no_test_examples = len(y_test)\n",
        "  try:\n",
        "    while True:\n",
        "        temp_test_accuracy = sess.run(accuracy, feed_dict={handle:test_string_handle})\n",
        "        total_test_accuracy += temp_test_accuracy*batch_size\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "    \n",
        "  print('Testing accuracy is {}'.format(total_test_accuracy/no_test_examples)) \n",
        "  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "---------------------------\n",
            "Training accuracy is 0.9239272727272727\n",
            "Validation accuracy is 0.9648\n",
            "Epoch 2\n",
            "---------------------------\n",
            "Training accuracy is 0.9762181818181818\n",
            "Validation accuracy is 0.9874\n",
            "Epoch 3\n",
            "---------------------------\n",
            "Training accuracy is 0.9831090909090909\n",
            "Validation accuracy is 0.9934\n",
            "Epoch 4\n",
            "---------------------------\n",
            "Training accuracy is 0.9873272727272727\n",
            "Validation accuracy is 0.9932\n",
            "Epoch 5\n",
            "---------------------------\n",
            "Training accuracy is 0.9900909090909091\n",
            "Validation accuracy is 0.9946\n",
            "Epoch 6\n",
            "---------------------------\n",
            "Training accuracy is 0.9920727272727272\n",
            "Validation accuracy is 0.9966\n",
            "Epoch 7\n",
            "---------------------------\n",
            "Training accuracy is 0.9931636363636364\n",
            "Validation accuracy is 0.996\n",
            "Epoch 8\n",
            "---------------------------\n",
            "Training accuracy is 0.9941272727272727\n",
            "Validation accuracy is 0.9988\n",
            "Epoch 9\n",
            "---------------------------\n",
            "Training accuracy is 0.9947454545454546\n",
            "Validation accuracy is 0.9966\n",
            "Epoch 10\n",
            "---------------------------\n",
            "Training accuracy is 0.9960363636363636\n",
            "Validation accuracy is 0.9992\n",
            "Testing the model --------\n",
            "Testing accuracy is 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dd1MLSz1VtqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "* https://www.tensorflow.org/api_docs/python/tf/data/Iterator#from_string_handle\n",
        "\n",
        "*  https://www.tensorflow.org/guide/datasets\n",
        "*  https://docs.google.com/presentation/d/16kHNtQslt-yuJ3w8GIx-eEH6t_AvFeQOchqGRFpAD7U/edit#slide=id.g254d08e080_0_141\n",
        "\n",
        "*  https://github.com/tensorflow/tensorflow/issues/2919"
      ]
    }
  ]
}